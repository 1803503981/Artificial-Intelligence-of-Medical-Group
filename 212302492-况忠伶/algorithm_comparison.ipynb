{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b22cd65",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745b85ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(108,\n",
       " Index(['性别', '发病节气', '劳累', '噪音', '耳机使用', '锻炼', '压力', '感染', '抽烟', '饮酒',\n",
       "        ...\n",
       "        'INR', 'APTT', 'FIB', 'D-二聚体', 'TT', '年龄', '病程（月）', 'TEQ', 'THI',\n",
       "        'VAS'],\n",
       "       dtype='object', length=107))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"D:\\学习资料\\项目资料\\耳鸣中医知识图谱\\neo4j\\数据\\新耳鸣数据纯数字.csv\",encoding=\"gbk\")\n",
    "x = data.drop(\"中医辨证\",axis=1)\n",
    "y = data[\"中医辨证\"] #0肾精亏损1肝火上扰2痰火郁结3脾胃亏虚4风热侵袭\n",
    "columns = data.columns ## 列名称\n",
    "features_name=data.columns[:-1]\n",
    "classes=np.unique(y)\n",
    "len(columns),features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895c4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      中医辨证   type\n",
      "0        0  train\n",
      "1        0  train\n",
      "2        0  train\n",
      "3        2  train\n",
      "4        3  train\n",
      "...    ...    ...\n",
      "1260     2  train\n",
      "1261     4  train\n",
      "1262     1  train\n",
      "1263     3  train\n",
      "1264     3  train\n",
      "\n",
      "[1265 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='中医辨证', ylabel='count'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 20013 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 21307 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 36776 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 35777 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 20013 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 21307 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 36776 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "E:\\ProgramFiles\\Anaconda\\anaconda\\envs\\kzl2\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 35777 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "train_label= data[['中医辨证']]\n",
    "train_label['type'] = 'train'\n",
    "label_all = pd.concat([train_label],axis=0)\n",
    "print(label_all)\n",
    "sns.countplot(x='中医辨证',hue='type', data=label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bb5773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.04166667, 1.        , ..., 0.45      , 0.18      ,\n",
       "        0.1875    ],\n",
       "       [0.        , 0.04166667, 1.        , ..., 0.35      , 0.22      ,\n",
       "        0.09375   ],\n",
       "       [0.        , 0.08333333, 1.        , ..., 0.55      , 0.42      ,\n",
       "        0.25      ],\n",
       "       ...,\n",
       "       [1.        , 0.        , 1.        , ..., 0.8       , 0.44      ,\n",
       "        0.28125   ],\n",
       "       [1.        , 0.        , 1.        , ..., 0.75      , 0.46      ,\n",
       "        0.28125   ],\n",
       "       [0.        , 0.        , 1.        , ..., 0.6       , 0.44      ,\n",
       "        0.21875   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "minMaxs = preprocessing.MinMaxScaler()\n",
    "data_minMax = minMaxs.fit_transform(x)\n",
    "data_minMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01f29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def getKFoldData(data,fileName):\n",
    "    df=data.sample(frac=1)\n",
    "    x = data.drop(\"中医辨证\",axis=1)\n",
    "    y = data[\"中医辨证\"]\n",
    "    minMaxs = preprocessing.MinMaxScaler()\n",
    "    data_minMax = minMaxs.fit_transform(x)\n",
    "    # kfold = KFold(n_splits=5,random_state=666,shuffle=True)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=999)\n",
    "    split_i=0\n",
    "    for train_index, test_index  in kfold.split(data_minMax,y):\n",
    "        split_i=split_i+1\n",
    "        # print(\"test index: \", test_index)\n",
    "        X_train,X_test=data_minMax[train_index],data_minMax[test_index]\n",
    "        y_train,y_test=y[train_index],y[test_index]\n",
    "        # print(Train[\"中医辨证\"].value_counts(),Test[\"中医辨证\"].value_counts())\n",
    "        # Train.to_csv(f'./data/五折训练集纯数字{split_i}.csv')\n",
    "        # Test.to_csv(f'./data/五折测试集纯数字{split_i}.csv')\n",
    "getKFoldData(data,'五折测试集纯数字')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418500d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54998ede",
   "metadata": {},
   "source": [
    "## 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725bb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "minMaxs = preprocessing.MinMaxScaler()\n",
    "data_minMax = minMaxs.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749854c2",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ea2889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.75      , 1.        , ..., 0.55      , 0.18      ,\n",
       "        0.15625   ],\n",
       "       [1.        , 0.54166667, 1.        , ..., 0.25      , 0.08      ,\n",
       "        0.0625    ],\n",
       "       [1.        , 0.        , 1.        , ..., 0.55      , 0.28      ,\n",
       "        0.09375   ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.        , ..., 0.4       , 0.22      ,\n",
       "        0.15625   ],\n",
       "       [1.        , 0.54166667, 1.        , ..., 0.45      , 0.32      ,\n",
       "        0.09375   ],\n",
       "       [1.        , 0.        , 1.        , ..., 0.55      , 0.16      ,\n",
       "        0.09375   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data_minMax,y,test_size=0.3,random_state = 0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90a0cb",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6c8a71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold_Score_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e346b370038d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#决策树实例化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mKFold_Score_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DecisionTreeClassifier'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'KFold_Score_new' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# model = tree.DecisionTreeClassifier(max_depth= 8, min_samples_leaf= 2, min_samples_split= 7, random_state= 66,class_weight=\"balanced\") #决策树实例化\n",
    "model = tree.DecisionTreeClassifier(class_weight=\"balanced\") #决策树实例化\n",
    "\n",
    "KFold_Score_new(data,model,'DecisionTreeClassifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(class_weight=\"balanced\") #决策树实例化\n",
    "model.fit(X_train,y_train)\n",
    "y_pre=model.predict(X_train)\n",
    "y_pre_test=model.predict(X_test)\n",
    "y_score = model.predict_proba(X_test)\n",
    "\n",
    "y_pre,y_pre_test\n",
    "y_test_binarize = label_binarize(y_pre_test, classes=classes)\n",
    "auc_score1 = roc_auc_score(y_test, y_test_binarize,average='weighted',multi_class = 'ovo')\n",
    "auc_score2 = roc_auc_score(y_test, y_score,average='weighted',multi_class = 'ovo')\n",
    "\n",
    "print(auc_score1,auc_score2)\n",
    "\n",
    "y_test_binarize,y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc300180",
   "metadata": {},
   "source": [
    "# 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# model = RandomForestClassifier(bootstrap=False, max_features=0.452660081319747, min_impurity_decrease=0,min_samples_leaf=6,max_depth= 6, \n",
    "#                        min_samples_split=10, min_weight_fraction_leaf=0,criterion='gini',n_jobs=1,n_estimators =512,\n",
    "#                               warm_start=True,class_weight='balanced')\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "KFold_Score_new(data,model,'RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa142c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d32b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# model = svm.SVC(C=10, class_weight='balanced', probability=True,kernel='linear')\n",
    "model = svm.SVC(class_weight='balanced',probability=True)\n",
    "KFold_Score_new(data,model,'SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d67ba",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "model = MLPClassifier(early_stopping=True,solver='sgd')\n",
    "KFold_Score_new(data,model,'MLPClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dba593",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729625c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "model = KNeighborsClassifier(weights='distance')\n",
    "KFold_Score_new(data,model,'KNeighborsClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c47fc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        68\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.97      0.95      0.96        39\n",
      "           3       1.00      0.89      0.94        54\n",
      "           4       0.97      0.94      0.95        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.95      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        68\n",
      "           1       0.94      1.00      0.97        61\n",
      "           2       0.97      0.90      0.93        39\n",
      "           3       0.98      0.96      0.97        54\n",
      "           4       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.97       253\n",
      "   macro avg       0.97      0.96      0.97       253\n",
      "weighted avg       0.97      0.97      0.97       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        68\n",
      "           1       0.97      1.00      0.98        61\n",
      "           2       0.93      0.95      0.94        39\n",
      "           3       0.96      0.96      0.96        54\n",
      "           4       1.00      0.97      0.98        31\n",
      "\n",
      "    accuracy                           0.97       253\n",
      "   macro avg       0.97      0.97      0.97       253\n",
      "weighted avg       0.97      0.97      0.97       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        68\n",
      "           1       0.95      0.97      0.96        62\n",
      "           2       0.97      0.89      0.93        38\n",
      "           3       0.96      0.98      0.97        54\n",
      "           4       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.96      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        67\n",
      "           1       0.91      0.97      0.94        62\n",
      "           2       0.95      0.92      0.94        39\n",
      "           3       0.98      0.98      0.98        54\n",
      "           4       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.96      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "         0        1        2        3        4         评价指标\n",
      "0  0.98576  0.98024  0.98178  0.98576  0.99366     accuracy\n",
      "1  0.97412  0.93828  0.95794  0.97784  0.98042    precision\n",
      "2  0.97336  0.98380  0.92252  0.95558  0.96772       recall\n",
      "3  0.97342  0.96042  0.93952  0.96604  0.97396           f1\n",
      "4  0.99028  0.97914  0.99252  0.99396  0.99730  specificity\n",
      "5  0.99266  0.99254  0.97702  0.98800  0.99282      roc_auc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "model = KNeighborsClassifier(weights='distance')\n",
    "KFold_Score_new(data,model,'KNeighborsClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee25a48-8070-4315-9efe-b59415cad354",
   "metadata": {},
   "source": [
    "# naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a9cdd9-f2ea-4f1b-bcaf-e85066db770f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94        68\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.97      0.92      0.95        39\n",
      "           3       0.96      0.89      0.92        54\n",
      "           4       0.97      0.94      0.95        31\n",
      "\n",
      "    accuracy                           0.93       253\n",
      "   macro avg       0.94      0.93      0.94       253\n",
      "weighted avg       0.93      0.93      0.93       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98        68\n",
      "           1       0.92      1.00      0.96        61\n",
      "           2       0.97      0.90      0.93        39\n",
      "           3       0.98      0.96      0.97        54\n",
      "           4       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.97      0.96      0.96       253\n",
      "weighted avg       0.97      0.96      0.96       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        68\n",
      "           1       0.94      1.00      0.97        61\n",
      "           2       0.90      0.95      0.92        39\n",
      "           3       0.96      0.94      0.95        54\n",
      "           4       1.00      0.90      0.95        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.95      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        68\n",
      "           1       0.95      0.98      0.97        62\n",
      "           2       0.97      0.87      0.92        38\n",
      "           3       0.95      0.98      0.96        54\n",
      "           4       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.95      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94        67\n",
      "           1       0.91      0.97      0.94        62\n",
      "           2       0.95      0.92      0.94        39\n",
      "           3       0.93      1.00      0.96        54\n",
      "           4       1.00      0.97      0.98        31\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.95      0.95      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "         0        1        2        3        4         评价指标\n",
      "0  0.97944  0.97550  0.97944  0.98102  0.99128     accuracy\n",
      "1  0.97096  0.92344  0.95312  0.95616  0.98042    precision\n",
      "2  0.95264  0.98048  0.91214  0.95556  0.94836       recall\n",
      "3  0.96126  0.95106  0.93150  0.95526  0.96380           f1\n",
      "4  0.98920  0.97392  0.99158  0.98792  0.99730  specificity\n",
      "5  0.99600  0.99608  0.99284  0.99458  0.99728      roc_auc\n"
     ]
    }
   ],
   "source": [
    "# 贝叶斯\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "KFold_Score_new(data,model,'naive_bayes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a10085-e9b4-40c0-83a2-c95a2cc44bde",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "964377a6-7e04-4f18-bbef-7ca403dd691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        68\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.95      0.95      0.95        39\n",
      "           3       0.98      0.91      0.94        54\n",
      "           4       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.95      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        68\n",
      "           1       0.97      1.00      0.98        61\n",
      "           2       1.00      0.95      0.97        39\n",
      "           3       0.98      0.98      0.98        54\n",
      "           4       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.98       253\n",
      "   macro avg       0.98      0.98      0.98       253\n",
      "weighted avg       0.98      0.98      0.98       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        68\n",
      "           1       0.98      0.95      0.97        61\n",
      "           2       0.88      0.92      0.90        39\n",
      "           3       0.93      0.96      0.95        54\n",
      "           4       1.00      0.94      0.97        31\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.95      0.95      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        68\n",
      "           1       0.97      0.98      0.98        62\n",
      "           2       0.97      0.92      0.95        38\n",
      "           3       0.98      0.98      0.98        54\n",
      "           4       0.97      1.00      0.98        31\n",
      "\n",
      "    accuracy                           0.97       253\n",
      "   macro avg       0.97      0.97      0.97       253\n",
      "weighted avg       0.97      0.97      0.97       253\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "指标报告               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96        67\n",
      "           1       0.95      0.95      0.95        62\n",
      "           2       0.95      0.95      0.95        39\n",
      "           3       0.95      0.98      0.96        54\n",
      "           4       1.00      0.97      0.98        31\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.96      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "         0        1        2        3        4         评价指标\n",
      "0  0.98576  0.98260  0.98262  0.98420  0.99366     accuracy\n",
      "1  0.97078  0.96136  0.94952  0.96360  0.98084    precision\n",
      "2  0.97634  0.96742  0.93806  0.96298  0.96772       recall\n",
      "3  0.97346  0.96426  0.94340  0.96288  0.97396           f1\n",
      "4  0.98920  0.98748  0.99066  0.98996  0.99730  specificity\n",
      "5  0.99714  0.99710  0.99440  0.99470  0.99722      roc_auc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "KFold_Score_new(data,model,'LogisticRegression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccda37",
   "metadata": {},
   "source": [
    "# 评价指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039b9a6",
   "metadata": {},
   "source": [
    "## 图算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc94a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"./outData新数据/递增结果/400递增结果.csv\",encoding=\"gbk\")#,encoding=\"gbk\"\n",
    "data=data.drop(\"Unnamed: 0\",axis=1)\n",
    "features_name=data.columns\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,confusion_matrix,classification_report,roc_auc_score,roc_curve,auc\n",
    "\n",
    "y_test=data[\"测试集证型\"]\n",
    "y_pre_test=data[\"出现次数最多证型\"]\n",
    "y_score = label_binarize(y_pre_test, classes=[0,1,2,3,4])\n",
    "auc=roc_auc_score(y_test, y_score,average='weighted',multi_class = 'ovo')\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c17a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=data[\"测试集证型\"]\n",
    "y_pre_test=data[\"出现次数最多证型\"]\n",
    "y_score = label_binarize(y_pre_test, classes=[0,1,2,3,4])\n",
    "auc=roc_auc_score(y_test, y_score,average='weighted',multi_class = 'ovo')\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowlege_graph_Score(data,\"多知识\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.unique(y)\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f97c4",
   "metadata": {},
   "source": [
    "### knowlege_graph_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9058ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,confusion_matrix,classification_report,roc_auc_score,roc_curve,auc\n",
    "\n",
    "def knowlege_graph_Score(data,filename):\n",
    "    testScores=[]\n",
    "    aucScores=[]\n",
    "    f1Scores=[]\n",
    "    precisions_test=[]\n",
    "    recalls=[]\n",
    "    speScores=[]\n",
    "    kappas=[]\n",
    "    #计算每一折\n",
    "    accScore=np.zeros((5, 5))\n",
    "    precisionScore=np.zeros((5, 5))\n",
    "    recallScore=np.zeros((5, 5))\n",
    "    f1Score=np.zeros((5, 5))\n",
    "    aucScore=np.zeros((5, 5))\n",
    "    speScore=np.zeros((5, 5))\n",
    "    df=data.sample(frac=1)\n",
    "    x = data.drop(\"中医辨证\",axis=1)\n",
    "    y = data[\"中医辨证\"]\n",
    "    classes=np.unique(y)\n",
    "\n",
    "    split_i=0\n",
    "    CIdata=pd.DataFrame()\n",
    "    CIdata['分类']=[\"肾精亏损\",\"肝火上扰\",\"痰火郁结\",\"脾胃亏虚\",\"风热侵袭\"]\n",
    "    CIname='F1'\n",
    "    for item in range(0,5):\n",
    "        print(item)\n",
    "        knowledge_graph_data=pd.read_csv(f\"./outData新数据/{filename}/{item}中医辨证分类结果数字.csv\",encoding=\"gbk\")\n",
    "        # y_test=knowledge_graph_data[\"Patient1中医辨证\"]#无权重\n",
    "        # y_test=knowledge_graph_data[\"测试集证型\"]#图结果、无知识\n",
    "        y_pre_test=knowledge_graph_data[\"出现次数最多证型\"]\n",
    "        y_score = label_binarize(y_pre_test, classes=classes)\n",
    "        '''ROC曲线'''\n",
    "        # roc_multiMethod(f\"{filename}\",y_test,y_score,classes)\n",
    "        '''CI置信区间'''\n",
    "        # CI=confidenceInterval(y_test,y_score,CIname) #0肾精亏虚1肝火上扰2痰火郁结3脾胃虚弱4风热侵袭\n",
    "        # CIdata['折数'f'{split_i}'+'_c1']=CI['c1']\n",
    "        # CIdata['折数'f'{split_i}'+'_c2']=CI['c2']\n",
    "        # print(CIdata)\n",
    "        # CIdata.to_csv(f'./modelsMulti/results/{filename}/CI_{CIname}.csv',encoding= \"utf_8_sig\",index= False)\n",
    "        '''计算每一类的ROC'''\n",
    "        y_test_binarize = label_binarize(y_test, classes=classes)\n",
    "        # 设置种类\n",
    "        n_classes = y_test_binarize.shape[1]\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        sum_TP=0\n",
    "        confusion = confusion_matrix(y_test,y_pre_test,labels=classes)\n",
    "        # print(\"kappa\",kappa(confusion))\n",
    "        for i in range(n_classes):\n",
    "            print(i)\n",
    "            sum_TP += confusion[i,i]\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_binarize[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            # print(roc_auc)\n",
    "        # print(\"特征重要性\",ferature_importance(model,title,isFalse))\n",
    "        print(\"指标报告\",classification_report(y_test, y_pre_test))\n",
    "        metrics_result = []\n",
    "        for i in range(n_classes):\n",
    "            # 逐步获取 真阳，假阳，真阴，假阴四个指标，并计算三个参数\n",
    "            ALL = np.sum(confusion)\n",
    "            # 对角线上是正确预测的\n",
    "            TP = confusion[i, i]\n",
    "            # 列加和减去正确预测是该类的假阳\n",
    "            FP = np.sum(confusion[:, i]) - TP\n",
    "            # 行加和减去正确预测是该类的假阴\n",
    "            FN = np.sum(confusion[i, :]) - TP\n",
    "            # 全部减去前面三个就是真阴\n",
    "            TN = sum_TP - TP\n",
    "            TN1 = ALL-TP-FP-FN\n",
    "            #accuracy\n",
    "            ACC=round((TP+TN1)/float(ALL), 4)\n",
    "            #Precision\n",
    "            P=TP/float(TP+FP)\n",
    "            PRE=round(P, 4)\n",
    "            #Recall\\sensitivity\n",
    "            R=TP/float(TP+FN)\n",
    "            REC=round(R,4)\n",
    "            #specificity\n",
    "            SPC=round(TN1/float(TN1+FP),4)\n",
    "            #F1\n",
    "            F1=round((2*P*R)/(P+R),4)\n",
    "            #auc\n",
    "            AUC=round(roc_auc[i],4)\n",
    "            accScore[i][split_i]=ACC\n",
    "            precisionScore[i][split_i]=PRE\n",
    "            recallScore[i][split_i]=REC\n",
    "            f1Score[i][split_i]=F1\n",
    "            speScore[i][split_i]=SPC\n",
    "            aucScore[i][split_i]=AUC\n",
    "        split_i=split_i+1\n",
    "        #macro:求取每一类的F值之后求平均值\n",
    "        #micro:每一类的预测结果都加起来之后再计算查准率、查全率、F值\n",
    "        #weighted:将各类别的F值乘以该类在总样本中的占比进行加权计算\n",
    "    testScores.append(accScore.mean(axis=1))\n",
    "    precisions_test.append(precisionScore.mean(axis=1))\n",
    "    recalls.append(recallScore.mean(axis=1)) #召回率/灵敏度\n",
    "    speScores.append(speScore.mean(axis=1))\n",
    "    f1Scores.append(f1Score.mean(axis=1))\n",
    "    aucScores.append(aucScore.mean(axis=1))\n",
    "    # print(accScore,precisionScore,recallScore,f1Score,speScore,aucScore)\n",
    "    # kappas.append(kappa)\n",
    "    #二维转为一维数组\n",
    "    # names = 'testScores precisions_test recalls speScores f1Scores aucScores'.split()\n",
    "    testScores=np.array(testScores)\n",
    "    testScores=testScores.flatten()\n",
    "    precisions_test=np.array(precisions_test)\n",
    "    precisions_test=precisions_test.flatten()\n",
    "    recalls=np.array(recalls)\n",
    "    recalls=recalls.flatten()\n",
    "    speScores=np.array(speScores)\n",
    "    speScores=speScores.flatten()\n",
    "    f1Scores=np.array(f1Scores)\n",
    "    f1Scores=f1Scores.flatten()\n",
    "    aucScores=np.array(aucScores)\n",
    "    aucScores=aucScores.flatten()\n",
    "    pd_score=pd.DataFrame(np.array([testScores,precisions_test,recalls,f1Scores,speScores,aucScores]))\n",
    "    pd_names=pd.DataFrame(np.array([\"accuracy\",\"precision\",\"recall\",\"f1\",\"specificity\",\"aucScores\"]))\n",
    "    pd_score[\"评价指标\"]=pd_names\n",
    "    pd_score.to_csv(f'./outData新数据/算法对比/{filename}五折多分类评价指标.csv',mode = 'a',encoding= \"utf_8_sig\",index= False)\n",
    "    print(pd_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebde13d",
   "metadata": {},
   "source": [
    "## KFold_Score_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38126efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,confusion_matrix,classification_report,roc_auc_score,roc_curve,auc\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def KFold_Score_new(data,model,fileName):\n",
    "    testScores=[]\n",
    "    aucScores=[]\n",
    "    f1Scores=[]\n",
    "    precisions_test=[]\n",
    "    recalls=[]\n",
    "    speScores=[]\n",
    "    kappas=[]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    #计算每一折\n",
    "    accScore=np.zeros((5, 5))\n",
    "    precisionScore=np.zeros((5, 5))\n",
    "    recallScore=np.zeros((5, 5))\n",
    "    f1Score=np.zeros((5, 5))\n",
    "    aucScore=np.zeros((5, 5))\n",
    "    speScore=np.zeros((5, 5))\n",
    "    df=data.sample(frac=1)\n",
    "    x = data.drop(\"中医辨证\",axis=1)\n",
    "    y = data[\"中医辨证\"]\n",
    "    classes=np.unique(y)\n",
    "    '''数据归一化'''\n",
    "    minMaxs = preprocessing.MinMaxScaler()\n",
    "    data_minMax = minMaxs.fit_transform(x)\n",
    "    '''五折划分数据集'''\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=999)\n",
    "    split_i=0\n",
    "    CIdata=pd.DataFrame()\n",
    "    CIdata['分类']=[\"肾精亏损\",\"肝火上扰\",\"痰火郁结\",\"脾胃亏虚\",\"风热侵袭\"]\n",
    "    CIname='F1'\n",
    "    for train_index, test_index  in kfold.split(data_minMax,y):\n",
    "        # print(\"test index: \", test_index)\n",
    "        X_train,X_test=data_minMax[train_index],data_minMax[test_index]\n",
    "        y_train,y_test=y[train_index],y[test_index]    \n",
    "        sample_weight=compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "        eval_set = [(X_train, y_train), (X_test,y_test)]\n",
    "        model.fit(X_train, y_train)\n",
    "        #fit函数不支持class_weight\n",
    "        # model.fit(X_train,y_train,early_stopping_rounds=10,eval_set=eval_set,verbose=0,sample_weight=sample_weight)\n",
    "        y_pre=model.predict(X_train)\n",
    "        y_pre_test=model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        '''ROC曲线'''\n",
    "        # roc_multiMethod(f\"{fileName}\",y_test,y_score,classes)        \n",
    "        # CI=confidenceInterval(y_test,y_pre_test,CIname) #0肾精亏损1肝火上扰2痰火郁结3脾胃亏虚4风热侵袭\n",
    "        # CIdata['折数'f'{split_i}'+'_c1']=CI['c1']\n",
    "        # CIdata['折数'f'{split_i}'+'_c2']=CI['c2']\n",
    "        # print(CIdata)\n",
    "        # CIdata.to_csv(f'./modelsMulti/results/{fileName}/CI_{CIname}.csv',encoding= \"utf_8_sig\",index= False)\n",
    "        # print(\"accuracytest_score\",accuracy_score(y_test,y_pre_test),accuracy_score(y_train,y_pre))\n",
    "        # 计算每一类的ROC\n",
    "        y_test_binarize = label_binarize(y_test, classes=classes)\n",
    "        # 设置种类\n",
    "        n_classes = y_test_binarize.shape[1]\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        sum_TP=0\n",
    "        confusion = confusion_matrix(y_test,y_pre_test,labels=classes)\n",
    "        # print(\"kappa\",kappa(confusion))\n",
    "        for i in range(n_classes):\n",
    "            print(i)\n",
    "            sum_TP += confusion[i,i]\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_binarize[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # print(\"特征重要性\",ferature_importance(model,title,isFalse))\n",
    "        print(\"指标报告\",classification_report(y_test, y_pre_test))\n",
    "        metrics_result = []\n",
    "        for i in range(n_classes):\n",
    "            # 逐步获取 真阳，假阳，真阴，假阴四个指标，并计算三个参数\n",
    "            ALL = np.sum(confusion)\n",
    "            # 对角线上是正确预测的\n",
    "            TP = confusion[i, i]\n",
    "            # 列加和减去正确预测是该类的假阳\n",
    "            FP = np.sum(confusion[:, i]) - TP\n",
    "            # 行加和减去正确预测是该类的假阴\n",
    "            FN = np.sum(confusion[i, :]) - TP\n",
    "            # 全部减去前面三个就是真阴\n",
    "            TN = sum_TP - TP\n",
    "            TN1 = ALL-TP-FP-FN\n",
    "            #accuracy\n",
    "            ACC=round((TP+TN1)/float(ALL), 4)\n",
    "            #Precision\n",
    "            P=TP/float(TP+FP)\n",
    "            PRE=round(P, 4)\n",
    "            #Recall\\sensitivity\n",
    "            R=TP/float(TP+FN)\n",
    "            REC=round(R,4)\n",
    "            #specificity\n",
    "            SPC=round(TN1/float(TN1+FP),4)\n",
    "            #F1\n",
    "            F1=round((2*P*R)/(P+R),4)\n",
    "            #auc\n",
    "            AUC=round(roc_auc[i],4)\n",
    "            accScore[i][split_i]=ACC\n",
    "            precisionScore[i][split_i]=PRE\n",
    "            recallScore[i][split_i]=REC\n",
    "            f1Score[i][split_i]=F1\n",
    "            speScore[i][split_i]=SPC\n",
    "            aucScore[i][split_i]=AUC\n",
    "            \n",
    "        split_i=split_i+1\n",
    "        #macro:求取每一类的F值之后求平均值\n",
    "        #micro:每一类的预测结果都加起来之后再计算查准率、查全率、F值\n",
    "        #weighted:将各类别的F值乘以该类在总样本中的占比进行加权计算\n",
    "    testScores.append(accScore.mean(axis=1))\n",
    "    precisions_test.append(precisionScore.mean(axis=1))\n",
    "    recalls.append(recallScore.mean(axis=1)) #召回率/灵敏度\n",
    "    speScores.append(speScore.mean(axis=1))\n",
    "    f1Scores.append(f1Score.mean(axis=1))\n",
    "    aucScores.append(aucScore.mean(axis=1))\n",
    "    # print(accScore,precisionScore,recallScore,f1Score,speScore,aucScore)\n",
    "    # kappas.append(kappa)\n",
    "    #二维转为一维数组\n",
    "    # names = 'testScores precisions_test recalls speScores f1Scores aucScores'.split()\n",
    "    testScores=np.array(testScores)\n",
    "    testScores=testScores.flatten()\n",
    "    precisions_test=np.array(precisions_test)\n",
    "    precisions_test=precisions_test.flatten()\n",
    "    recalls=np.array(recalls)\n",
    "    recalls=recalls.flatten()\n",
    "    speScores=np.array(speScores)\n",
    "    speScores=speScores.flatten()\n",
    "    f1Scores=np.array(f1Scores)\n",
    "    f1Scores=f1Scores.flatten()\n",
    "    aucScores=np.array(aucScores)\n",
    "    aucScores=aucScores.flatten()\n",
    "    pd_score=pd.DataFrame(np.array([testScores,precisions_test,recalls,f1Scores,speScores,aucScores]))\n",
    "    pd_names=pd.DataFrame(np.array([\"accuracy\",\"precision\",\"recall\",\"f1\",\"specificity\",\"roc_auc\"]))\n",
    "    pd_score[\"评价指标\"]=pd_names\n",
    "    pd_score.to_csv(f'./outData新数据/算法对比/{fileName}五折多分类评价指标.csv',mode = 'a',encoding= \"utf_8_sig\",index= False)\n",
    "    print(pd_score)\n",
    "    # print(testScores,precisions_test,recalls,speScores,f1Scores,aucScores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd22666",
   "metadata": {},
   "source": [
    "## kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8720c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def kappa(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    n = np.sum(matrix)\n",
    "    sum_po = 0\n",
    "    sum_pe = 0\n",
    "    for i in range(len(matrix[0])):\n",
    "        sum_po += matrix[i][i]\n",
    "        row = np.sum(matrix[i, :])\n",
    "        col = np.sum(matrix[:, i])\n",
    "        sum_pe += row * col\n",
    "    po = sum_po / n\n",
    "    pe = sum_pe / (n * n)\n",
    "    # print(po, pe)\n",
    "    return (po - pe) / (1 - pe)\n",
    "# matrix=confusion_matrix(y_test,y_pre_test,labels=[0,1,2,3,4])\n",
    "# print(kappa(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca01efd",
   "metadata": {},
   "source": [
    "# 置信区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "model = RandomForestClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df=data.sample(frac=1)\n",
    "x = data.drop(\"中医辨证\",axis=1)\n",
    "y = data[\"中医辨证\"]\n",
    "classes=np.unique(y)\n",
    "minMaxs = preprocessing.MinMaxScaler()\n",
    "data_minMax = minMaxs.fit_transform(x)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=999)\n",
    "for train_index, test_index  in kfold.split(data_minMax,y):\n",
    "    # print(\"test index: \", test_index)\n",
    "    X_train,X_test=data_minMax[train_index],data_minMax[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]    \n",
    "    sample_weight=compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    eval_set = [(X_train, y_train), (X_test,y_test)]\n",
    "    model.fit(X_train, y_train)\n",
    "    #fit函数不支持class_weight\n",
    "    # model.fit(X_train,y_train,early_stopping_rounds=10,eval_set=eval_set,verbose=0,sample_weight=sample_weight)\n",
    "    y_pre=model.predict(X_train)\n",
    "    y_pre_test=model.predict(X_test)\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    print(\"折数：\")\n",
    "    CI=confidenceInterval(y_test,y_score,'ACC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_graph_data=pd.read_csv(f\"./outData新数据/图结果/5折_1分类结果.csv\",encoding=\"gbk\")\n",
    "knowledge_graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16064a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45174d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=[0,1,2,3,4]\n",
    "classes=[0]\n",
    "filename=\"图结果\"\n",
    "bootstraps=100\n",
    "result = []\n",
    "ALL_data=pd.DataFrame(columns=[\"测试集证型\",\"出现次数最多证型\"])\n",
    "for item in range(0,5):\n",
    "    knowledge_graph_data=pd.read_csv(f\"./outData新数据/图结果/5折_{item}分类结果.csv\",encoding=\"gbk\")\n",
    "    ALL_data = pd.concat([ALL_data, knowledge_graph_data],ignore_index=True)\n",
    "    y_test=knowledge_graph_data[\"测试集证型\"]\n",
    "    y_pre_test=knowledge_graph_data[\"出现次数最多证型\"]\n",
    "fold_size=len(y_pre_test)\n",
    "prevalence=[]\n",
    "syndrome_datas=[]\n",
    "df=pd.DataFrame(columns=[\"测试集证型\",\"出现次数最多证型\"])\n",
    "'''计算每个证型结果占比'''\n",
    "for i in range(len(n_classes)):\n",
    "    syndrome_data = ALL_data[ALL_data[\"测试集证型\"]==i] #各个证型结果\n",
    "    # print(syndrome_data)\n",
    "    ratio=len(syndrome_data)/len(y_test)\n",
    "    prevalence.append(ratio)\n",
    "    syndrome_datas.append(syndrome_data)\n",
    "'''从每个证型中按原比例随机抽取结果'''\n",
    "for k in range(bootstraps):\n",
    "    for m in range(len(n_classes)):\n",
    "        new_data = syndrome_datas[m].sample(n = int(fold_size * prevalence[m]), replace=True)\n",
    "        df = pd.concat([df, new_data],ignore_index=True)\n",
    "    df=df.sample(frac=1.0)# 打乱顺序\n",
    "    print(df)\n",
    "    y_test_new=df[\"测试集证型\"].astype('int64')\n",
    "    y_pre_test_new=df[\"出现次数最多证型\"].astype('int64')    \n",
    "    '''使用混淆矩阵计算所有指标'''\n",
    "    confusion=confusion_matrix(y_test_new,y_pre_test_new,labels=n_classes)\n",
    "    print(confusion)\n",
    "    metrics_result = []\n",
    "    sum_TP=0\n",
    "    for j in range(len(n_classes)):\n",
    "        sum_TP+=confusion[j, j];\n",
    "    '''逐步获取 真阳，假阳，真阴，假阴四个指标，并计算三个参数'''\n",
    "    for v in range(len(n_classes)):\n",
    "        ALL = np.sum(confusion)\n",
    "        # 对角线上是正确预测的\n",
    "        TP = confusion[v, v]\n",
    "        # 列加和减去正确预测是该类的假阳\n",
    "        FP = np.sum(confusion[:, v]) - TP\n",
    "        # 行加和减去正确预测是该类的假阴\n",
    "        FN = np.sum(confusion[v, :]) - TP\n",
    "        # 全部减去前面三个就是真阴\n",
    "        TN = sum_TP - TP\n",
    "        TN1 = ALL-TP-FP-FN\n",
    "        #accuracy\n",
    "        ACC=round((TP+TN1)/float(ALL), 4)\n",
    "        #Precision\n",
    "        P=TP/float(TP+FP)\n",
    "        PRE=round(P, 4)\n",
    "        #Recall\\sensitivity\n",
    "        R=TP/float(TP+FN)\n",
    "        REC=round(R,4)\n",
    "        #specificity\n",
    "        SPC=round(TN1/float(TN1+FP),4)\n",
    "        #F1\n",
    "        F1=round((2*P*R)/(P+R),4)\n",
    "        metrics_result.append([F1])#精准度、敏感度、特异性,F1, PRE, REC, SPC, F1, AUC\n",
    "    result.append(np.transpose(metrics_result))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfe42b",
   "metadata": {},
   "source": [
    "## 使用预测值和预测结果计算CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=[0,1,2,3,4]\n",
    "classes=[0]\n",
    "filename=\"图结果\"\n",
    "bootstraps=100\n",
    "result = []\n",
    "for item in range(len(classes)):\n",
    "    df=pd.DataFrame(columns=[\"测试集证型\",\"出现次数最多证型\"])\n",
    "    knowledge_graph_data=pd.read_csv(f\"./outData新数据/{filename}/5折_{item}分类结果.csv\",encoding=\"gbk\")\n",
    "    y_test=knowledge_graph_data[\"测试集证型\"]\n",
    "    y_pre_test=knowledge_graph_data[\"出现次数最多证型\"]\n",
    "    fold_size=len(y_pre_test)\n",
    "    prevalence=[]\n",
    "    syndrome_datas=[]\n",
    "    '''计算每个证型结果占比'''\n",
    "    for i in range(len(n_classes)):\n",
    "        syndrome_data = knowledge_graph_data[knowledge_graph_data[\"测试集证型\"]==i] #各个证型结果\n",
    "        # print(syndrome_data)\n",
    "        ratio=len(syndrome_data)/len(y_test)\n",
    "        prevalence.append(ratio)\n",
    "        syndrome_datas.append(syndrome_data)\n",
    "    '''从每个证型中按原比例随机抽取结果'''\n",
    "    for k in range(bootstraps):\n",
    "        for m in range(len(n_classes)):\n",
    "            new_data = syndrome_datas[m].sample(n = int(fold_size * prevalence[m]), replace=True)\n",
    "            df = pd.concat([df, new_data],ignore_index=True)\n",
    "        df=df.sample(frac=1.0)# 打乱顺序\n",
    "        print(df)\n",
    "        y_test_new=df[\"测试集证型\"].astype('int64')\n",
    "        y_pre_test_new=df[\"出现次数最多证型\"].astype('int64')    \n",
    "        '''使用混淆矩阵计算所有指标'''\n",
    "        confusion=confusion_matrix(y_test_new,y_pre_test_new,labels=n_classes)\n",
    "        print(confusion)\n",
    "        metrics_result = []\n",
    "        sum_TP=0\n",
    "        for j in range(len(n_classes)):\n",
    "            sum_TP+=confusion[j, j];\n",
    "        '''逐步获取 真阳，假阳，真阴，假阴四个指标，并计算三个参数'''\n",
    "        for v in range(len(n_classes)):\n",
    "            ALL = np.sum(confusion)\n",
    "            # 对角线上是正确预测的\n",
    "            TP = confusion[v, v]\n",
    "            # 列加和减去正确预测是该类的假阳\n",
    "            FP = np.sum(confusion[:, v]) - TP\n",
    "            # 行加和减去正确预测是该类的假阴\n",
    "            FN = np.sum(confusion[v, :]) - TP\n",
    "            # 全部减去前面三个就是真阴\n",
    "            TN = sum_TP - TP\n",
    "            TN1 = ALL-TP-FP-FN\n",
    "            #accuracy\n",
    "            ACC=round((TP+TN1)/float(ALL), 4)\n",
    "            #Precision\n",
    "            P=TP/float(TP+FP)\n",
    "            PRE=round(P, 4)\n",
    "            #Recall\\sensitivity\n",
    "            R=TP/float(TP+FN)\n",
    "            REC=round(R,4)\n",
    "            #specificity\n",
    "            SPC=round(TN1/float(TN1+FP),4)\n",
    "            #F1\n",
    "            F1=round((2*P*R)/(P+R),4)\n",
    "            metrics_result.append([F1])#精准度、敏感度、特异性,F1, PRE, REC, SPC, F1, AUC\n",
    "        result.append(np.transpose(metrics_result))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ee168",
   "metadata": {},
   "source": [
    "## confidenceInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dbf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''计算置信区间CI'''\n",
    "statistics = np.array(result)\n",
    "statistics = statistics.reshape(100, -1).T\n",
    "cis = []\n",
    "hangshu = np.shape(statistics)[0]\n",
    "alpha = 0.95  # 是设定的可信区间, 可以理解为置信度; 数值是百分数,取值范围(0,1)\n",
    "for i in range(0, hangshu):\n",
    "    line = statistics[i, :]\n",
    "    n = len(line)\n",
    "    ci = stats.t.interval(alpha, n - 1, np.mean(line), stats.sem(line))\n",
    "    ci1 = np.array(ci)\n",
    "    cis.append(ci1)\n",
    "cis = np.array(cis)\n",
    "cis_csv = pd.DataFrame(cis, columns=['c1','c2'])\n",
    "cis_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20513843",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''计算置信区间CI'''\n",
    "def confidenceInterval(solution,prediction,CIname):\n",
    "    statistics = bootstrap_tinitus(solution, prediction, [0,1,2,3,4],len(prediction),CIname)\n",
    "    statistics = statistics.reshape(100, -1).T\n",
    "    cis = []\n",
    "    hangshu = np.shape(statistics)[0]\n",
    "    alpha = 0.95  # 是设定的可信区间, 可以理解为置信度; 数值是百分数,取值范围(0,1)\n",
    "    for i in range(0, hangshu):\n",
    "        line = statistics[i, :]\n",
    "        n = len(line)\n",
    "        ci = stats.t.interval(alpha, n - 1, np.mean(line), stats.sem(line))\n",
    "        ci1 = np.array(ci)\n",
    "        cis.append(ci1)\n",
    "    cis = np.array(cis)\n",
    "    cis_csv = pd.DataFrame(cis, columns=['c1','c2'])\n",
    "    # cis_csv.to_csv(f'./modelsMulti/results/CI_{CIname}.csv',mode = 'a', encoding= \"utf_8_sig\",index= False,header=None)\n",
    "    return cis_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef12b6",
   "metadata": {},
   "source": [
    "## bootstrap_tinitus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5674b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_tinitus1(y_test, y_pre_test, n_classes, fold_size, CIname, bootstraps = 100):\n",
    "    result = []\n",
    "    prevalence=[]\n",
    "    syndrome_datas=[]\n",
    "    '''计算每个证型结果占比'''\n",
    "    for i in range(len(n_classes)):\n",
    "        syndrome_data = y_test[y_test[\"测试集证型\"]==i] #各个证型结果\n",
    "        # print(syndrome_data)\n",
    "        ratio=len(syndrome_data)/len(y_test)\n",
    "        prevalence.append(ratio)\n",
    "        syndrome_datas.append(syndrome_data)\n",
    "    '''从每个证型中按原比例随机抽取结果'''\n",
    "    for k in range(bootstraps):\n",
    "        for m in range(len(n_classes)):\n",
    "            new_data = syndrome_datas[m].sample(n = int(fold_size * prevalence[m]), replace=True)\n",
    "            df = pd.concat([df, new_data],ignore_index=True)\n",
    "        df=df.sample(frac=1.0)# 打乱顺序\n",
    "        print(df)\n",
    "        y_test_new=df[\"测试集证型\"].astype('int64')\n",
    "        y_pre_test_new=df[\"出现次数最多证型\"].astype('int64')    \n",
    "        '''使用混淆矩阵计算所有指标'''\n",
    "        confusion=confusion_matrix(y_test_new,y_pre_test_new,labels=n_classes)\n",
    "        print(confusion)\n",
    "        metrics_result = []\n",
    "        sum_TP=0\n",
    "        for j in range(len(n_classes)):\n",
    "            sum_TP+=confusion[j, j];\n",
    "        '''逐步获取 真阳，假阳，真阴，假阴四个指标，并计算三个参数'''\n",
    "        for v in range(len(n_classes)):\n",
    "            ALL = np.sum(confusion)\n",
    "            # 对角线上是正确预测的\n",
    "            TP = confusion[v, v]\n",
    "            # 列加和减去正确预测是该类的假阳\n",
    "            FP = np.sum(confusion[:, v]) - TP\n",
    "            # 行加和减去正确预测是该类的假阴\n",
    "            FN = np.sum(confusion[v, :]) - TP\n",
    "            # 全部减去前面三个就是真阴\n",
    "            TN = sum_TP - TP\n",
    "            TN1 = ALL-TP-FP-FN\n",
    "            #accuracy\n",
    "            ACC=round((TP+TN1)/float(ALL), 4)\n",
    "            #Precision\n",
    "            P=TP/float(TP+FP)\n",
    "            PRE=round(P, 4)\n",
    "            #Recall\\sensitivity\n",
    "            R=TP/float(TP+FN)\n",
    "            REC=round(R,4)\n",
    "            #specificity\n",
    "            SPC=round(TN1/float(TN1+FP),4)\n",
    "            #F1\n",
    "            F1=round((2*P*R)/(P+R),4)\n",
    "            metrics_result.append([F1])#精准度、敏感度、特异性,F1, PRE, REC, SPC, F1, AUC\n",
    "        result.append(np.transpose(metrics_result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''有放回的抽样bootstraps次\n",
    "抽样样本大小为fold_size'''\n",
    "\n",
    "def bootstrap_tinitus(y_test, y_pred_prob, classes, fold_size, CIname, bootstraps = 100):\n",
    "    result = []\n",
    "    df = pd.DataFrame(columns=['y', 'pred0'])\n",
    "    df['y'] = y_test\n",
    "    args = locals()\n",
    "    for h in range(len(classes)):\n",
    "        df['pred'+str(h)] = y_pred_prob[:,h]\n",
    "    #动态赋值\n",
    "    for j in range(len(classes)):\n",
    "        args[\"df_{}\".format(j)] = df[df.y == j]\n",
    "    prevalence=[]\n",
    "    for item in range(len(classes)):\n",
    "        prevalence.append(len(args[\"df_{}\".format(item)]) / len(df)) #y=j在y_test中占的比例\n",
    "\n",
    "    for i in range(bootstraps):\n",
    "        #随机抽取n行，replace是否为有放回抽样，True:有放回抽样，False:未放回抽样\n",
    "        for m in range(len(classes)):\n",
    "            args[\"sample{}\".format(m)] = args[\"df_{}\".format(m)].sample(n = int(fold_size * prevalence[m]), replace=True)\n",
    "            # print(\"sample:\",m,args[\"sample{}\".format(m)])\n",
    "        y_sample = np.concatenate([args[\"sample{}\".format(0)].y.values, args[\"sample{}\".format(1)].y.values, args[\"sample{}\".format(2)].y.values,\n",
    "                                   args[\"sample{}\".format(3)].y.values, args[\"sample{}\".format(4)].y.values,\n",
    "                                  ])\n",
    "\n",
    "        for n in range(len(classes)):\n",
    "            args[\"pred_sample{}\".format(n)]=np.vstack((args[\"sample{}\".format(n)].pred0.values,args[\"sample{}\".format(n)].pred1.values))\n",
    "            args[\"pred_sample{}\".format(n)]=np.vstack((args[\"pred_sample{}\".format(n)],args[\"sample{}\".format(n)].pred2.values))\n",
    "            args[\"pred_sample{}\".format(n)]=np.vstack((args[\"pred_sample{}\".format(n)],args[\"sample{}\".format(n)].pred3.values))\n",
    "            args[\"pred_sample{}\".format(n)]=np.vstack((args[\"pred_sample{}\".format(n)],args[\"sample{}\".format(n)].pred4.values))\n",
    "            args[\"pred_sample{}\".format(n)]=np.array(args[\"pred_sample{}\".format(n)])\n",
    "            args[\"pred_sample{}\".format(n)]=args[\"pred_sample{}\".format(n)].T\n",
    "            # print(\"pred_sample:\",n,args[\"pred_sample{}\".format(n)])\n",
    "\n",
    "        # print(\"pred_sample0\",len(args[\"pred_sample{}\".format(0)]))\n",
    "        pred_sample_1=args[\"pred_sample{}\".format(0)]\n",
    "        for k in range(1,len(classes)):\n",
    "            pred_sample_1=np.vstack((pred_sample_1,args[\"pred_sample{}\".format(k)]))\n",
    "            # print(\"pred_sample_1:\",k,len(pred_sample_1))\n",
    "        #将预测概率换为预测值\n",
    "        y_pre_test=np.argmax(pred_sample_1,axis=1)\n",
    "        # 计算每一类的ROC\n",
    "        y_test_binarize = label_binarize(y_sample, classes=classes)\n",
    "        # 设置种类\n",
    "        n_classes = y_test_binarize.shape[1]\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        confusion = confusion_matrix(y_sample,y_pre_test,labels=classes)\n",
    "        sum_TP=0\n",
    "        for j in range(n_classes):\n",
    "            sum_TP+=confusion[j, j];\n",
    "        metrics_result = []\n",
    "        for v in range(n_classes):\n",
    "            fpr[v], tpr[v], _ = roc_curve(y_test_binarize[:, v], pred_sample_1[:, v])\n",
    "            roc_auc[v] = auc(fpr[v], tpr[v])\n",
    "            # 逐步获取 真阳，假阳，真阴，假阴四个指标，并计算三个参数\n",
    "            ALL = np.sum(confusion)\n",
    "            # 对角线上是正确预测的\n",
    "            TP = confusion[v, v]\n",
    "            # 列加和减去正确预测是该类的假阳\n",
    "            FP = np.sum(confusion[:, v]) - TP\n",
    "            # 行加和减去正确预测是该类的假阴\n",
    "            FN = np.sum(confusion[v, :]) - TP\n",
    "            # 全部减去前面三个就是真阴\n",
    "            TN = sum_TP - TP\n",
    "            TN1 = ALL-TP-FP-FN\n",
    "            #accuracy\n",
    "            ACC=round((TP+TN1)/float(ALL), 4)\n",
    "            #Precision\n",
    "            P=TP/float(TP+FP)\n",
    "            PRE=round(P, 4)\n",
    "            #Recall\\sensitivity\n",
    "            R=TP/float(TP+FN)\n",
    "            REC=round(R,4)\n",
    "            #specificity\n",
    "            SPC=round(TN1/float(TN1+FP),4)\n",
    "            #F1\n",
    "            F1=round((2*P*R)/(P+R),4)\n",
    "            #auc\n",
    "            AUC=round(roc_auc[v],4)\n",
    "            metrics_result.append([F1])#精准度、敏感度、特异性,F1, PRE, REC, SPC, F1, AUC\n",
    "        result.append(np.transpose(metrics_result))\n",
    "    # print(len(result))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4060ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc697275",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_means = []\n",
    "for  in range(10000):\n",
    "   bootsample = data.sample(90, replace=True)\n",
    "   mean = bootsample[bootsample['性别'] == 1]['体温'].mean()\n",
    "   boot_means.append(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec5c09",
   "metadata": {},
   "source": [
    "# roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b60d36",
   "metadata": {},
   "source": [
    "## roc_multiMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "def roc_multiMethod(title,y_test,y_score,classes):\n",
    "\n",
    "    # 将标签二值化\n",
    "    y_label = label_binarize(y_test, classes=classes)\n",
    "    # 设置种类\n",
    "    n_classes = y_label.shape[1]\n",
    "    y_test = label_binarize(y_test, classes=classes)\n",
    "    # print(y_score)\n",
    "    # 计算每一类的ROC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area（方法二）\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # # Compute macro-average ROC curve and ROC area（方法一）\n",
    "    # # First aggregate all false positive rates\n",
    "    # all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # # Then interpolate all ROC curves at this points\n",
    "    # mean_tpr = np.zeros_like(all_fpr)\n",
    "    # for i in range(n_classes):\n",
    "    #     mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    # # Finally average it and compute AUC\n",
    "    # mean_tpr /= n_classes\n",
    "    # fpr[\"macro\"] = all_fpr\n",
    "    # tpr[\"macro\"] = mean_tpr\n",
    "    # roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    lw=2\n",
    "    #图片显示中文\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] =False #减号unicode编码\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    # plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "    #          label='macro-average ROC curve (area = {0:0.2f})'\n",
    "    #                ''.format(roc_auc[\"macro\"]),\n",
    "    #          color='navy', linestyle=':', linewidth=4)\n",
    "    # names=[\"肾精亏损\", \"肝火上扰\", \"痰火郁结\",\"脾胃亏虚\",\"风热侵袭\"]\n",
    "    # colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    # for i, color in zip(range(n_classes), colors):\n",
    "    #     plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "    #              label='ROC of class {0} (area = {1:0.2f})'\n",
    "    #              ''.format(names[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title+'ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(f'./{title}ROC曲线.png')#保存图片\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d2c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29c8532c",
   "metadata": {},
   "source": [
    "## 随着特征增加AUC的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test,y_pre_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c820ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(0,5):\n",
    "    print(item)\n",
    "    knowledge_graph_data=pd.read_csv(f\"./outData新数据/图结果/5折_{item}分类结果.csv\",encoding=\"gbk\")\n",
    "    # y_test=knowledge_graph_data[\"Patient1中医辨证\"]\n",
    "    # y_pre_test=knowledge_graph_data[\"前20出现次数最多\"]\n",
    "    y_test=knowledge_graph_data[\"测试集证型\"]\n",
    "    y_pre_test=knowledge_graph_data[\"出现次数最多证型\"]\n",
    "    # y_test_new=df[\"测试集证型\"].astype('int64')\n",
    "    # y_pre_test_new=df[\"出现次数最多证型\"].astype('int64')\n",
    "    y_pre_binarize = label_binarize(y_pre_test, classes=classes)\n",
    "    auc_score = roc_auc_score(y_test, y_pre_binarize,average='weighted',multi_class = 'ovo')\n",
    "    print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330151a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
